x testing auto regression
x more memory efficient preprocessing
x shuffle data during training
x adding export batch size (only if dynamic axes is not working)
x add gradient accumulation
x columns to use for training and inference as config parameters
x compile model using torch.compile or thunder (just released)
x implement early stopping if validation error doesn't improve over x epochs
x fix empty printing during training
x file formats other than csv? -> yes, parquet, for compression
x make sequifier usable in one go if config_paths are default
x replace batch resizing with dynamic axes -> not possible unless I give up dict as model input
x add exporting to pickle and gpu inference/(make onnx a flag)
x address non identical outputs on inference --|> doesn't seem to be the case now

add sampling from distribution

invert --on-preprocessed arg
make autoregression true by default

make underscore and dash usage consistent
make query/loc consistent
systematize hparams or self access for hyperparameters in train.py
use numpy reduce where possible
remove unnecessary n_sequences return value from starmap
address torch depreciation warning
reorder config and class attributes logically
reorder functions and methods logically


add dynamic preprocessing on training -> less necessary now that parquet is the default
