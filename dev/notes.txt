x testing auto regression
x more memory efficient preprocessing
x shuffle data during training
x adding export batch size (only if dynamic axes is not working)
x add gradient accumulation
x columns to use for training and inference as config parameters
x compile model using torch.compile or thunder (just released)
x implement early stopping if validation error doesn't improve over x epochs
x fix empty printing during training
x file formats other than csv? -> yes, parquet, for compression
x make sequifier usable in one go if config_paths are default
x replace batch resizing with dynamic axes -> not possible unless I give up dict as model input
x add exporting to pickle and gpu inference/(make onnx a flag)
x address non identical outputs on inference --|> doesn't seem to be the case now
(x) add dynamic preprocessing on training -> less necessary now that parquet is the default
x make autoregression true by default
x invert --on-preprocessed arg
x use numpy reduce where possible
x remove unnecessary n_sequences return value from starmap
x add sampling from distribution for categorical target variables
x add dropout option on inference
x reorder functions and methods logically
x systematize hparams or self access for hyperparameters in train.py
x reorder config and class attributes logically
x multi target preprocessing, training & inference
x loss weights in multi target setting

make underscore and dash usage in files/folders consistent

add decoder only architecture
learnable positional encoding
address torch depreciation warning

make fewer config values optional

look into device in get_probs_preds

make losses invariant over batch sizes/training runs
