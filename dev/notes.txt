x testing auto regression
x more memory efficient preprocessing
x shuffle data during training
x adding export batch size (only if dynamic axes is not working)
x add gradient accumulation
x columns to use for training and inference as config parameters
x compile model using torch.compile or thunder (just released)
x implement early stopping if validation error doesn't improve over x epochs
x fix empty printing during training
x file formats other than csv? -> yes, parquet, for compression
x make sequifier usable in one go if config_paths are default
x replace batch resizing with dynamic axes -> not possible unless I give up dict as model input
x add exporting to pickle and gpu inference/(make onnx a flag)
x address non identical outputs on inference --|> doesn't seem to be the case now
(x) add dynamic preprocessing on training -> less necessary now that parquet is the default
x make autoregression true by default
x invert --on-preprocessed arg
x use numpy reduce where possible
x remove unnecessary n_sequences return value from starmap
x add sampling from distribution for categorical target variables
x add dropout option on inference
x reorder functions and methods logically
x systematize hparams or self access for hyperparameters in train.py
x reorder config and class attributes logically
x multi target preprocessing, training & inference
x loss weights in multi target setting
x make underscore and dash usage in files/folders consistent
x learnable positional encoding
x make more config values optional
x look into device in get_probs_preds
x enable overwriting dd_config values in train and infer
x enable numbers of split groups smaller than 3
x output single predictions file
x enable autoregression continuation
x automatic scaling of real variables to [-1.0, 1.0] and inversion on inference
x log predicted class distributions in validation
make losses invariant over batch sizes/training runs
